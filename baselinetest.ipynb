{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, accuracy_score, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "your_datapath = ''\n",
    "\n",
    "#Define search space for number of trees in random forest and depth of trees\n",
    "num_trees_min = 64#default 64 \n",
    "num_trees_max = 128#default 128\n",
    "\n",
    "depth_min = 2\n",
    "depth_max = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_district_df_semiyearly(datapath, district_name):\n",
    "    \"\"\"\n",
    "    Function that creates a pandas dataframe for a single district with columns for the baseline model with semiyearly entries\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    datapath : string\n",
    "        Path to the datafolder\n",
    "    district_name : string\n",
    "        Name of the district\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    df : pandas dataframe\n",
    "    \"\"\"\n",
    "\n",
    "\t#Read all relevant datasets\n",
    "    prevalence_df = pd.read_csv(datapath + 'prevalence_estimates.csv', parse_dates=['date'])\n",
    "    covid_df = pd.read_csv(datapath + 'covid.csv', parse_dates=['date'])\n",
    "    ipc_df = pd.read_csv(datapath + 'ipc2.csv', parse_dates=['date'])\n",
    "    risk_df = pd.read_csv(datapath + 'FSNAU_riskfactors.csv', parse_dates=['date'])\n",
    "    production_df = pd.read_csv(datapath + 'production.csv', parse_dates=['date'])\n",
    "    \n",
    "    #Select data for specific district\n",
    "    prevalence_df = prevalence_df[prevalence_df['district']==district_name]\n",
    "    ipc_df = ipc_df[ipc_df['district']==district_name]\n",
    "    risk_df = risk_df[risk_df['district']==district_name]\n",
    "    production_df = production_df[production_df['district']==district_name]\n",
    "\n",
    "    risk_df = risk_df.groupby(pd.Grouper(key='date', freq='6M')).mean()\n",
    "    risk_df = risk_df.reset_index()\n",
    "    risk_df['date'] = risk_df['date'].apply(lambda x : x.replace(day=1))\n",
    "    \n",
    "    covid_df = covid_df.groupby(pd.Grouper(key='date', freq='6M')).sum()\n",
    "    covid_df = covid_df.reset_index()\n",
    "    covid_df['date'] = covid_df['date'].apply(lambda x : x.replace(day=1))\n",
    "    \n",
    "    production_df['cropdiv'] = production_df.count(axis=1)\n",
    "    \n",
    "    #Sort dataframes on date\n",
    "    prevalence_df.sort_values('date', inplace=True)\n",
    "    covid_df.sort_values('date', inplace=True)\n",
    "    ipc_df.sort_values('date', inplace=True)\n",
    "    risk_df.sort_values('date', inplace=True)\n",
    "    production_df.sort_values('date', inplace=True)\n",
    "\n",
    "    #Merge dataframes, only joining on current or previous dates as to prevent data leakage\n",
    "    df = pd.merge_asof(left=prevalence_df, right=ipc_df, direction='backward', on='date')\n",
    "    df = pd.merge_asof(left=df, right=production_df, direction='backward', on='date')\n",
    "    df = pd.merge_asof(left=df, right=risk_df, direction='backward', on='date')\n",
    "    df = pd.merge_asof(left=df, right=covid_df, direction='backward', on='date')\n",
    "    \n",
    "    #Calculate prevalence 6lag\n",
    "    df['prevalence_6lag'] = df['GAM Prevalence'].shift(1)\n",
    "    df['next_prevalence'] = df['GAM Prevalence'].shift(-1)\n",
    "    \n",
    "    #Select needed columns\n",
    "    df = df[['date', 'district', 'GAM Prevalence', 'next_prevalence', 'prevalence_6lag', 'new_cases', 'ndvi_score', 'phase3plus_perc', 'cropdiv', 'total population']]\n",
    "    df.columns = ['date', 'district', 'prevalence', 'next_prevalence', 'prevalence_6lag', 'covid', 'ndvi', 'ipc', 'cropdiv', 'population']\n",
    "    \n",
    "    #Add month column\n",
    "    df['month'] = df['date'].dt.month\n",
    "    \n",
    "    #Add target variable: increase for next month prevalence (boolean)\n",
    "    increase = [False if x[1]<x[0] else True for x in list(zip(df['prevalence'], df['prevalence'][1:]))]\n",
    "    increase.append(False)\n",
    "    df['increase'] = increase\n",
    "    df.iloc[-1, df.columns.get_loc('increase')] = np.nan #No info on next month\n",
    "    \n",
    "    #Add target variable: increase for next month prevalence (boolean)\n",
    "    increase_numeric = [x[1] - x[0] for x in list(zip(df['prevalence'], df['prevalence'][1:]))]\n",
    "    increase_numeric.append(0)\n",
    "    df['increase_numeric'] = increase_numeric\n",
    "    df.iloc[-1, df.columns.get_loc('increase_numeric')] = np.nan #No info on next month\n",
    "    \n",
    "    df.loc[(df.date < pd.to_datetime('2020-03-01')), 'covid'] = 0\n",
    "    \n",
    "    return(df)\n",
    "    \n",
    "    \n",
    "#Function that combines the semiyearly dataset (from the function make_district_df_semiyearly) of all districts\n",
    "def make_combined_df_semiyearly(datapath):\n",
    "    \"\"\"\n",
    "    Function that creates a pandas dataframe for all districts with columns for the baseline model with semiyearly entries\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    datapath : string\n",
    "        Path to the datafolder\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    df : pandas dataframe\n",
    "    \"\"\"\n",
    "\n",
    "    prevdf = pd.read_csv(datapath + 'prevalence_estimates.csv', parse_dates=['date'])\n",
    "    districts = prevdf['district'].unique()\n",
    "    \n",
    "    df_list = []\n",
    "    for district in districts:\n",
    "        district_df = make_district_df_semiyearly(datapath, district)\n",
    "        district_df['district'] = district\n",
    "        df_list.append(district_df)\n",
    "        \n",
    "    df = pd.concat(df_list, ignore_index=True)\n",
    "    df['district_encoded'] = df['district'].astype('category').cat.codes\n",
    "\n",
    "    return df\n",
    "\n",
    "#Function that returns every possible subset (except the empty set) of the input list l\n",
    "def subsets (l):\n",
    "    subset_list = []\n",
    "    for i in range(len(l) + 1):\n",
    "        for j in range(i):\n",
    "            subset_list.append(l[j: i])\n",
    "    return subset_list\n",
    "\n",
    "\n",
    "\n",
    "'''------------SECTION DATAFRAME CREATION--------------'''\n",
    "#Create the dataframe for all districts\n",
    "df = make_combined_df_semiyearly(your_datapath)\n",
    "\n",
    "#Drop every row with missing values\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "#Sort dataframe on date and reset the index\n",
    "df.sort_values('date', inplace=True)\n",
    "df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "#Drop disctricts with less than 7 observations: 'Burco', 'Saakow', 'Rab Dhuure', 'Baydhaba', 'Afmadow'\n",
    "df.drop(df[df['district'].isin(['Burco', 'Saakow', 'Rab Dhuure', 'Baydhaba', 'Afmadow'])].index, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create empty list to store model scores\n",
    "parameter_scores = []\n",
    "\n",
    "#Define target and explanatory variables\n",
    "X = df.drop(columns = ['increase', 'increase_numeric', 'date', 'district', 'prevalence', 'next_prevalence']) #Note that these columns are dropped, the remaining columns are used as explanatory variables\n",
    "y = df['next_prevalence'].values\n",
    "\n",
    "for num_trees in range(num_trees_min, num_trees_max):\n",
    "    \n",
    "    for depth in range(depth_min, depth_max):\n",
    "        \n",
    "        #Investigate every subset of explanatory variables\n",
    "        for features in subsets(X.columns):\n",
    "        \n",
    "            #First CV split. The 99 refers to the first 3 observations for the 33 districts in the data.\n",
    "            Xtrain = X[:99][features].copy().values\n",
    "            ytrain = y[:99]\n",
    "            Xtest = X[99:132][features].copy().values\n",
    "            ytest = y[99:132]\n",
    "\n",
    "            #Create a RandomForestRegressor with the selected hyperparameters and random state 0.\n",
    "            clf = RandomForestRegressor(n_estimators=num_trees, max_depth=depth, random_state=0)\n",
    "\n",
    "            #Fit to the training data\n",
    "            clf.fit(Xtrain, ytrain)\n",
    "\n",
    "            #Make a prediction on the test data\n",
    "            predictions = clf.predict(Xtest)\n",
    "\n",
    "            #Calculate mean absolute error\n",
    "            MAE1 = mean_absolute_error(ytest, predictions)\n",
    "\n",
    "\n",
    "            #Second CV split. The 132 refers to the first 4 observations for the 33 districts in the data.\n",
    "            Xtrain = X[:132][features].copy().values\n",
    "            ytrain = y[:132]\n",
    "            Xtest = X[132:165][features].copy().values\n",
    "            ytest = y[132:165]\n",
    "\n",
    "            #Create a RandomForestRegressor with the selected hyperparameters and random state 0.\n",
    "            clf = RandomForestRegressor(n_estimators=num_trees, max_depth=depth, random_state=0)\n",
    "\n",
    "            #Fit to the training data\n",
    "            clf.fit(Xtrain, ytrain)\n",
    "\n",
    "            #Make a prediction on the test data\n",
    "            predictions = clf.predict(Xtest)\n",
    "\n",
    "            #Calculate mean absolute error\n",
    "            MAE2 = mean_absolute_error(ytest, predictions)\n",
    "\n",
    "            #Calculate the mean MAE over the two folds\n",
    "            mean_MAE = (MAE1 + MAE2)/2\n",
    "\n",
    "            #Store the mean MAE together with the used hyperparameters in list \n",
    "            parameter_scores.append((mean_MAE, num_trees, depth, features))\n",
    "\n",
    "#Sort the models based on score and retrieve the hyperparameters of the best model\n",
    "parameter_scores.sort(key=lambda x: x[0])\n",
    "best_model_score = parameter_scores[0][0]\n",
    "best_model_trees = parameter_scores[0][1]\n",
    "best_model_depth = parameter_scores[0][2]\n",
    "best_model_columns = list(parameter_scores[0][3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predictions)\n",
    "print(ytest)\n",
    "print(r2_score(ytest, predictions))\n",
    "print(df)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[best_model_columns].values\n",
    "y = df['next_prevalence'].values\n",
    "\n",
    "#If there is only one explanatory variable, the values need to be reshaped for the model\n",
    "if len(best_model_columns) == 1:\n",
    "\tX = X.reshape(-1, 1)\n",
    "\n",
    "#Peform evaluation on full data\n",
    "Xtrain = X[:165]\n",
    "ytrain = y[:165]\n",
    "Xtest = X[165:]\n",
    "ytest = y[165:]\n",
    "\n",
    "clf = RandomForestRegressor(n_estimators=best_model_trees, max_depth=best_model_depth, random_state=0)\n",
    "clf.fit(Xtrain, ytrain)\n",
    "predictions = clf.predict(Xtest)\n",
    "\n",
    "#Calculate MAE\n",
    "MAE = mean_absolute_error(ytest, predictions)\n",
    "\n",
    "#Generate boolean values for increase or decrease in prevalence. 0 if next prevalence is smaller than current prevalence, 1 otherwise.\n",
    "increase           = [0 if x<y else 1 for x in df.iloc[165:]['next_prevalence'] for y in df.iloc[165:]['prevalence']]\n",
    "predicted_increase = [0 if x<y else 1 for x in predictions                      for y in df.iloc[165:]['prevalence']]\n",
    "\n",
    "#Calculate accuracy of predicted boolean increase/decrease\n",
    "acc = accuracy_score(increase, predicted_increase)\n",
    "\n",
    "#Print model parameters\n",
    "print('no. of trees: ' + str(best_model_trees) + '\\nmax_depth: ' + str(best_model_depth) + '\\ncolumns: ' + str(best_model_columns))\n",
    "\n",
    "#Print model scores\n",
    "print(MAE, acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_conflict = pd.read_csv('conflict.csv')\n",
    "df_covid = pd.read_csv('covid.csv')\n",
    "df_risk = pd.read_csv('FSNAU_riskfactors.csv')\n",
    "df_ipc= pd.read_csv('ipc.csv')\n",
    "df_ipc2 = pd.read_csv('ipc2.csv')\n",
    "df_prev = pd.read_csv('prevalence_estimates.csv')\n",
    "df_production = pd.read_csv('production.csv')\n",
    "print(df_conflict.head(20))\n",
    "print(df_covid.head(20))\n",
    "print(df_risk.head(20))\n",
    "print(df_ipc.head(20))\n",
    "print(df_ipc2.head(20))\n",
    "print(df_prev.head(20))\n",
    "print(df_production.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predictions)\n",
    "print(df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "164a7008b9cda7ef95f0c12403a5a42846dd595385533d0146ff7590ff7f8905"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
